# RAG：把文档向量化，基于向量实现真正的语义搜索 - 总结

对应代码：`chapter/6/rag-test/src/hello-rag.mjs`。环境变量：根目录 `.env` 中配置 `OPENAI_API_KEY`、`OPENAI_BASE_URL`、`MODEL_NAME`（大模型）、`EMBEDDINGS_MODEL_NAME`（嵌入模型，如 text-embedding-v3）；通过 `import "dotenv/config"` 加载。

---

## 一、为什么需要 RAG

- 大模型的知识边界取决于训练数据，无法获知训练截止日之后的信息或企业私有文档，却可能「自以为知道」并乱答，即**幻觉**。
- 思路：在把用户问题交给大模型前，先从**内部知识库**检索与问题相关的文档片段，作为背景知识放进 prompt，再让大模型基于这些内容生成回答，从而减少幻觉、提升可追溯性。

## 二、RAG 是什么

- **RAG = Retrieval（检索）- Augmented（增强）- Generation（生成）**
- 流程概括：根据用户 query 去知识库**检索**相关文档 → 将检索结果**增强**到 prompt 中 → 由大模型**生成**回答。
- 基于关键词的检索也算一种 RAG，但无法做**语义检索**（例如问「水果」要能关联到「苹果、香蕉」等），意义有限；要实现语义检索，需要**向量**。

## 三、为什么用向量做检索

- **关键词检索**：依赖字面匹配，无法表达「意思相近」；用户问「水果」时，难以稳定命中「苹果、香蕉」等相关文档。
- **向量与相似度**：把概念用高维向量表示（如几百维），通过**向量夹角**衡量语义相关性：夹角越小越相似，即**余弦相似度**。向量库中存的是文档（或文档块）的向量，查询时把 query 也向量化，用相似度找出最相关的文档。
- 向量维度由**嵌入模型**决定，虽无法像二维那样可视化，但「用夹角/余弦相似度判断相关性」的原理相同。

## 四、嵌入模型与向量存储

- **嵌入模型（Embedding Model）**：专门把文本（或图片、语音等）转成向量的模型，与**大语言模型（LLM）**不同，只做「知识 → 向量」的映射，不做生成；调用成本通常低于 LLM。
- **向量与文档的对应**：文档在向量化时，会在向量的**元信息**里记录来源（如原文、metadata），这样检索到相似向量后即可取回对应文档内容。
- **向量库**：存储向量及关联的文档信息；示例用 **MemoryVectorStore**（内存），适合小规模；生产可用 Pinecone、Chroma、Milvus 等持久化向量库。

## 五、RAG 流程与代码要点（hello-rag.mjs）

| 步骤 | 说明 |
|------|------|
| 大模型与嵌入 | **ChatOpenAI** 用于最终回答；**OpenAIEmbeddings** 用于把文档和 query 转成向量，需与向量库使用同一嵌入模型。 |
| 文档与 Document | 用 **Document**（`pageContent` + `metadata`）表示每个片段；多个 Document 组成待检索的知识库。 |
| 向量化入库 | **MemoryVectorStore.fromDocuments(documents, embeddings)**：用 embeddings 把 documents 转成向量并写入内存向量库。 |
| 检索器 | **vectorStore.asRetriever({ k: 3 })**：封装「按 query 检索最相关 k 条文档」的逻辑；k=3 表示返回相似度最高的 3 个 Document。 |
| 检索 | **retriever.invoke(question)**：将 question 向量化，在向量库中做相似度检索，返回最相关的 k 个 Document。 |
| 相似度评分（可选） | **vectorStore.similaritySearchWithScore(query, k)**：返回 `[Document, score][]`，score 为距离（越小越相似）；相似度可表示为 1 - score，便于打印或过滤。 |
| 增强 prompt | 把检索到的 Document 的 pageContent 拼成「故事片段」等上下文，插入到 system/user prompt 中，再调用 model.invoke(prompt) 生成回答。 |

- 整体流程：用户问题 → 嵌入模型向量化 → 向量库相似度检索 → 取回相关文档 → 拼进 prompt → 大模型基于这些文档生成回答。

## 六、环境与依赖

- **依赖**：`@langchain/openai`（ChatOpenAI、OpenAIEmbeddings）、`@langchain/core`（Document）、`@langchain/classic`（MemoryVectorStore）、`dotenv`。
- **环境变量**：`OPENAI_API_KEY`、`OPENAI_BASE_URL`（如千问兼容地址）、`MODEL_NAME`（大模型）、`EMBEDDINGS_MODEL_NAME`（嵌入模型）；`.env` 不提交，需自行配置。
- 示例中知识库为内置的故事片段（光光与东东），仅用于演示检索与增强；实际应用可替换为从文件、数据库等加载的文档并做切块后再向量化入库。

## 七、总结

- **RAG**：先检索知识库中与 query 相关的文档，再增强到 prompt 中，由大模型生成回答，用于缓解幻觉、利用私有/最新知识。
- **语义检索**：依赖**向量**；用**嵌入模型**把文档和 query 转成向量，通过**余弦相似度**在向量库中检索最相关文档；关键词检索无法实现语义层面的匹配。
- **代码要点**：Document 表示文档片段 → fromDocuments 向量化入库 → asRetriever(k) 得到检索器 → retriever.invoke(query) 取相关文档 → 拼入 prompt → 大模型生成；similaritySearchWithScore 可用于查看或过滤相似度。
- **扩展**：内部文档助手、知识库问答等场景，均可采用「文档切块 → 向量化入库 → 查询时检索 + 增强 prompt → 生成」的 RAG 流程。
